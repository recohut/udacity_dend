{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Role\": {\n",
      "        \"Path\": \"/\",\n",
      "        \"RoleName\": \"emr-serverless-job-role\",\n",
      "        \"RoleId\": \"AROAVVYXO24EXG5JQDKNV\",\n",
      "        \"Arn\": \"arn:aws:iam::390354360073:role/emr-serverless-job-role\",\n",
      "        \"CreateDate\": \"2022-06-17T18:37:33+00:00\",\n",
      "        \"AssumeRolePolicyDocument\": {\n",
      "            \"Version\": \"2012-10-17\",\n",
      "            \"Statement\": [\n",
      "                {\n",
      "                    \"Effect\": \"Allow\",\n",
      "                    \"Principal\": {\n",
      "                        \"Service\": \"emr-serverless.amazonaws.com\"\n",
      "                    },\n",
      "                    \"Action\": \"sts:AssumeRole\"\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "aws iam create-role --role-name emr-serverless-job-role --assume-role-policy-document '{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "      {\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Principal\": {\n",
    "          \"Service\": \"emr-serverless.amazonaws.com\"\n",
    "        },\n",
    "        \"Action\": \"sts:AssumeRole\"\n",
    "      }\n",
    "    ]\n",
    "  }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "aws iam put-role-policy --role-name emr-serverless-job-role --policy-name S3Access --policy-document '{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"ReadFromOutputAndInputBuckets\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::udacity-dend-redshift\",\n",
    "                \"arn:aws:s3:::udacity-dend-redshift/*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"WriteToOutputDataBucket\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:DeleteObject\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::udacity-dend-redshift/*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "aws iam put-role-policy --role-name emr-serverless-job-role --policy-name GlueAccess --policy-document '{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "      {\n",
    "        \"Sid\": \"GlueCreateAndReadDataCatalog\",\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Action\": [\n",
    "            \"glue:GetDatabase\",\n",
    "            \"glue:GetDataBases\",\n",
    "            \"glue:CreateTable\",\n",
    "            \"glue:GetTable\",\n",
    "            \"glue:GetTables\",\n",
    "            \"glue:GetPartition\",\n",
    "            \"glue:GetPartitions\",\n",
    "            \"glue:CreatePartition\",\n",
    "            \"glue:BatchCreatePartition\",\n",
    "            \"glue:GetUserDefinedFunctions\"\n",
    "        ],\n",
    "        \"Resource\": [\"*\"]\n",
    "      }\n",
    "    ]\n",
    "  }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%sh\n",
    "# aws emr-serverless create-application \\\n",
    "#   --type SPARK \\\n",
    "#   --name serverless-demo \\\n",
    "#   --release-label \"emr-6.6.0\" \\\n",
    "#     --initial-capacity '{\n",
    "#         \"DRIVER\": {\n",
    "#             \"workerCount\": 2,\n",
    "#             \"workerConfiguration\": {\n",
    "#                 \"cpu\": \"2vCPU\",\n",
    "#                 \"memory\": \"4GB\"\n",
    "#             }\n",
    "#         },\n",
    "#         \"EXECUTOR\": {\n",
    "#             \"workerCount\": 10,\n",
    "#             \"workerConfiguration\": {\n",
    "#                 \"cpu\": \"4vCPU\",\n",
    "#                 \"memory\": \"8GB\"\n",
    "#             }\n",
    "#         }\n",
    "#     }' \\\n",
    "#     --maximum-capacity '{\n",
    "#         \"cpu\": \"200vCPU\",\n",
    "#         \"memory\": \"200GB\",\n",
    "#         \"disk\": \"1000GB\"\n",
    "#     }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"application\": {\n",
      "        \"applicationId\": \"00f1li2klr0lrv09\",\n",
      "        \"name\": \"my-serverless-application\",\n",
      "        \"arn\": \"arn:aws:emr-serverless:us-east-1:390354360073:/applications/00f1li2klr0lrv09\",\n",
      "        \"releaseLabel\": \"emr-6.6.0\",\n",
      "        \"type\": \"Spark\",\n",
      "        \"state\": \"STARTED\",\n",
      "        \"stateDetails\": \"\",\n",
      "        \"initialCapacity\": {\n",
      "            \"Driver\": {\n",
      "                \"workerCount\": 1,\n",
      "                \"workerConfiguration\": {\n",
      "                    \"cpu\": \"4 vCPU\",\n",
      "                    \"memory\": \"16 GB\",\n",
      "                    \"disk\": \"20 GB\"\n",
      "                }\n",
      "            },\n",
      "            \"Executor\": {\n",
      "                \"workerCount\": 2,\n",
      "                \"workerConfiguration\": {\n",
      "                    \"cpu\": \"4 vCPU\",\n",
      "                    \"memory\": \"16 GB\",\n",
      "                    \"disk\": \"20 GB\"\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"maximumCapacity\": {\n",
      "            \"cpu\": \"400 vCPU\",\n",
      "            \"memory\": \"1600 GB\",\n",
      "            \"disk\": \"2000 GB\"\n",
      "        },\n",
      "        \"createdAt\": \"2022-06-12T12:59:08.796000+05:30\",\n",
      "        \"updatedAt\": \"2022-06-18T00:04:56.471000+05:30\",\n",
      "        \"tags\": {},\n",
      "        \"autoStartConfiguration\": {\n",
      "            \"enabled\": true\n",
      "        },\n",
      "        \"autoStopConfiguration\": {\n",
      "            \"enabled\": true,\n",
      "            \"idleTimeoutMinutes\": 15\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws emr-serverless get-application --application-id 00f1li2klr0lrv09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: S3_BUCKET=udacity-dend-redshift\n",
      "env: APPLICATION_ID=00f1li2klr0lrv09\n",
      "env: JOB_ROLE_ARN=arn:aws:iam::390354360073:role/emr-serverless-job-role\n"
     ]
    }
   ],
   "source": [
    "%env S3_BUCKET=udacity-dend-redshift\n",
    "%env APPLICATION_ID=00f1li2klr0lrv09\n",
    "%env JOB_ROLE_ARN=arn:aws:iam::390354360073:role/emr-serverless-job-role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing spark.cfg\n"
     ]
    }
   ],
   "source": [
    "%%writefile spark.cfg\n",
    "[S3]\n",
    "SOURCE_S3_PATH = s3a://udacity-dend-redshift/\n",
    "DEST_S3_PATH = s3a://udacity-dend-redshift/spark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spark_etl.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spark_etl.py\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "\n",
    "SOURCE_S3_BUCKET = \"s3://udacity-dend-redshift/\"\n",
    "DEST_S3_BUCKET = \"s3://udacity-dend-redshift/spark/\"\n",
    "\n",
    "\n",
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "\n",
    "def process_song_data(spark, input_data, output_data):\n",
    "    \"\"\" Process song data and create songs and artists table\n",
    "    \n",
    "        Arguments:\n",
    "            spark {object}: SparkSession object\n",
    "            input_data {object}: Source S3 endpoint\n",
    "            output_data {object}: Target S3 endpoint\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    # get filepath to song data file\n",
    "    song_data = os.path.join(input_data + 'song_data/*/*/*/*.json')\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data)\n",
    "    \n",
    "    # create temp view of song data for songplays table to join\n",
    "    df.createOrReplaceTempView(\"song_data_view\")\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select('song_id', 'title', 'artist_id', 'year', 'duration').distinct()\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.mode(\"overwrite\").partitionBy('year', 'artist_id')\\\n",
    "               .parquet(path=output_data + 'songs')\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select('artist_id', 'artist_name', 'artist_location',\\\n",
    "                              'artist_latitude', 'artist_longitude').distinct()\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.mode(\"overwrite\").parquet(path=output_data + 'artists')\n",
    "\n",
    "    \n",
    "def process_log_data(spark, input_data, output_data):\n",
    "    \"\"\" Process log data and create users, time and songplays table\n",
    "        Arguments:\n",
    "            spark {object}: SparkSession object\n",
    "            input_data {object}: Source S3 endpoint\n",
    "            output_data {object}: Target S3 endpoint\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    # get filepath to log data file\n",
    "    log_data = os.path.join(input_data + 'log_data/*/*/*.json')\n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data)\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = df.where(df['page'] == 'NextSong')\n",
    "\n",
    "    # extract columns for users table    \n",
    "    users_table = df.select('userId', 'firstName', 'lastName', 'gender', 'level').distinct()\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table.write.mode(\"overwrite\").parquet(path=output_data + 'users')\n",
    "\n",
    "    \"\"\"  \n",
    "    # create timestamp column from original timestamp column\n",
    "    get_timestamp = udf()\n",
    "    df = \n",
    "    # create datetime column from original timestamp column\n",
    "    get_datetime = udf()\n",
    "    df = \n",
    "    \"\"\"\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    df = df.withColumn('start_time', (df['ts']/1000).cast('timestamp'))\n",
    "    df = df.withColumn('weekday', date_format(df['start_time'], 'E'))\n",
    "    df = df.withColumn('year', year(df['start_time']))\n",
    "    df = df.withColumn('month', month(df['start_time']))\n",
    "    df = df.withColumn('week', weekofyear(df['start_time']))\n",
    "    df = df.withColumn('day', dayofmonth(df['start_time']))\n",
    "    df = df.withColumn('hour', hour(df['start_time']))\n",
    "    time_table = df.select('start_time', 'weekday', 'year', 'month',\\\n",
    "                           'week', 'day', 'hour').distinct()\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.mode('overwrite').partitionBy('year', 'month')\\\n",
    "              .parquet(path=output_data + 'time')\n",
    "    \n",
    "    # read in song data to use for songplays table\n",
    "    song_df = spark.sql(\"SELECT * FROM song_data_view\")\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    songplays_table = df.join(song_df, (df.song == song_df.title)\\\n",
    "                                       & (df.artist == song_df.artist_name)\\\n",
    "                                       & (df.length == song_df.duration), \"inner\")\\\n",
    "                        .distinct()\\\n",
    "                        .select('start_time', 'userId', 'level', 'song_id',\\\n",
    "                                'artist_id', 'sessionId','location','userAgent',\\\n",
    "                                df['year'].alias('year'), df['month'].alias('month'))\\\n",
    "                        .withColumn(\"songplay_id\", monotonically_increasing_id())\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write.mode(\"overwrite\").partitionBy('year', 'month')\\\n",
    "                   .parquet(path=output_data + 'songplays')\n",
    "\n",
    "    \n",
    "def main():\n",
    "    spark = create_spark_session()\n",
    "    input_data = SOURCE_S3_BUCKET\n",
    "    output_data = DEST_S3_BUCKET\n",
    "    \n",
    "    process_song_data(spark, input_data, output_data)    \n",
    "    process_log_data(spark, input_data, output_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./spark_etl.py to s3://udacity-dend-redshift/code/pyspark/spark/spark_etl.py\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./spark_etl.py s3://${S3_BUCKET}/code/pyspark/spark/spark_etl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"applicationId\": \"00f1li2klr0lrv09\",\n",
      "    \"jobRunId\": \"00f1pvv4cq5mtu01\",\n",
      "    \"arn\": \"arn:aws:emr-serverless:us-east-1:390354360073:/applications/00f1li2klr0lrv09/jobruns/00f1pvv4cq5mtu01\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "aws emr-serverless start-job-run \\\n",
    "    --application-id $APPLICATION_ID \\\n",
    "    --execution-role-arn $JOB_ROLE_ARN \\\n",
    "    --job-driver '{\n",
    "        \"sparkSubmit\": {\n",
    "            \"entryPoint\": \"s3://'${S3_BUCKET}'/code/pyspark/spark/spark_etl.py\",\n",
    "            \"sparkSubmitParameters\": \"--conf spark.driver.cores=1 --conf spark.driver.memory=3g --conf spark.executor.cores=4 --conf spark.executor.memory=3g --conf spark.executor.instances=10\"\n",
    "        }\n",
    "    }' \\\n",
    "    --configuration-overrides '{\n",
    "        \"monitoringConfiguration\": {\n",
    "            \"s3MonitoringConfiguration\": {\n",
    "                \"logUri\": \"s3://'${S3_BUCKET}'/logs/\"\n",
    "            }\n",
    "        }\n",
    "    }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://prod.us-east-1.appinfo.src/emr-serverless/390354360073/applications/00f1li2klr0lrv09/jobs/00f1pv6elvcfbs01/sparklogs/* ./logs --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[2]\").getOrCreate()\n",
    "df = spark.read.parquet('s3://udacity-dend-redshift/songplays/year=2018/month=11/*.parquet')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('env-spacy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "343191058819caea96d5cde1bd3b1a75b4807623ce2cda0e1c8499e39ac847e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
